{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b9a8f2c",
   "metadata": {},
   "source": [
    "# The Preamble to Descriptive Statistics\n",
    "\n",
    "- Change Font Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfc289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "    /* Increase the font size for all text */\n",
    "    body {\n",
    "        font-size: 20px;\n",
    "    }\n",
    "    \n",
    "    /* Optionally, you can also increase the font size for specific elements */\n",
    "    h1, h2, h3, h4, h5, h6 {\n",
    "        font-size: 18px; /* Adjust the size as needed */\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b8f994",
   "metadata": {},
   "source": [
    "## Setting Up Python\n",
    "1. Check if Python is istalled:\n",
    "    <br>**Windows**: Open the Command Prompt:\n",
    "    - Press Win + R to open the \"Run\" dialog.\n",
    "    - Type cmd and press Enter. \n",
    " \n",
    "    In the Command Prompt, type:\n",
    "    - python --version <br> or \n",
    "    \n",
    "    - python3 --version\n",
    "     \n",
    "This command will display the installed Python version if Python is installed. If Python is not installed, you'll typically see an error message.\n",
    "\n",
    "2. **MacOS**: Open the Terminal\n",
    "    - Press Command + Space to open Spotlight Search\n",
    "    - Type \"Terminal\" and press Enter.\n",
    "    \n",
    "    In the Terminal, type:\n",
    "    \n",
    "    - python --version <br> or\n",
    "    \n",
    "    - python3 --version\n",
    "    \n",
    "Similar to Windows, this command will display the installed Python version if Python is installed. <br> If Python is not installed, you'll typically see an error message.\n",
    "\n",
    "3. **Linux (Ubuntu/Debian-based)**: Open the Terminal\n",
    "    - You can usually find the Terminal application in the Applications menu or by searching for \"Terminal\".\n",
    "    \n",
    "    In the Terminal, type:\n",
    "    \n",
    "    - python --version <br> or\n",
    "    \n",
    "    - python3 --version <br>\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "<br>4. **Install Python**: First, ensure that Python is installed on your system. You can download the latest version of Python from the official website: [Python Downloads](https://www.python.org/downloads/). Follow the installation instructions for your operating system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6d9415",
   "metadata": {},
   "source": [
    "## Install Jupyter Notebook\n",
    "\n",
    "Once Python is installed, you can install Jupyter Notebook using pip, which is Python's package installer. Open your command line or terminal and run the following command: \n",
    "\n",
    "   - pip install jupyter\n",
    "\n",
    "After the installation is complete, start Jupyter Notebook by running on the command prompt:\n",
    "\n",
    "   - jupyter notebook\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a832beb",
   "metadata": {},
   "source": [
    "You may also install Jupyter Notebook by using Anaconda through the following steps:\n",
    "\n",
    "  - Download and install Anaconda from the [Anaconda Website](https://www.anaconda.com/download)\n",
    "  \n",
    "  - Add Anaconda to your system’s PATH\n",
    "  \n",
    "  - Launch the Anaconda Navigator.\n",
    "  \n",
    "  - Click on the “Install Jupyter Notebook” button\n",
    "  \n",
    "  - Once the installation is complete, click the “Launch” button to start Jupyter Notebook.\n",
    "  \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff10a0f0",
   "metadata": {},
   "source": [
    "## How to start jupyter notebook using the Anaconda Navigator\n",
    "\n",
    "- **Launch Anaconda Navigator**: Open Anaconda Navigator. You can usually find it in your applications or by searching for \"Anaconda Navigator\" in the Start menu (Windows) or Launchpad (macOS).\n",
    "\n",
    "- **Launch Jupyter Notebook**: In the Anaconda Navigator window, you'll see a list of applications. Find and click on the \"Jupyter Notebook\" icon. This will open a new tab in your default web browser, and Jupyter Notebook will start running.\n",
    "\n",
    "- **Create or Open a Notebook**: Once Jupyter Notebook is running, you can create a new notebook by clicking on the \"New\" button in the top-right corner and selecting \"Python 3\" (or any other available kernel). Alternatively, you can open an existing notebook by navigating to the directory where your notebook is located and clicking on it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38225de3",
   "metadata": {},
   "source": [
    "## Python Libraries\n",
    "In Python, libraries (also known as modules or packages) are collections of functions, classes, and variables that extend the capabilities of the core Python language. These libraries are designed to address specific tasks or domains, such as data manipulation, scientific computing, web development, machine learning, and more. By importing and using libraries in your Python code, you can leverage pre-written code to perform various tasks efficiently without having to write everything from scratch. Some popular Python libraries include NumPy, Pandas, Matplotlib, TensorFlow, Flask, and Django.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c47084",
   "metadata": {},
   "source": [
    "## Python Libraries for Descriptive Statistics\n",
    "There are several Python libraries commonly used for descriptive statistics, each offering various functionalities for analyzing and summarizing data. Here are some of the most popular ones:\n",
    "\n",
    "1. **NumPy**:\n",
    "\n",
    "- NumPy is a fundamental library for scientific computing in Python.\n",
    "- It provides support for arrays, matrices, and mathematical functions to operate on these data structures efficiently.\n",
    "- NumPy's `mean`, `median`, `std`, `var`, `min`, `max`, `sum`, and other functions can be used for descriptive statistics.\n",
    "\n",
    "2. **Pandas**:\n",
    "\n",
    "- Pandas is a powerful library for data manipulation and analysis.\n",
    "- It offers data structures like Series and DataFrame, which are well-suited for handling tabular data.\n",
    "- Pandas provides functions like `describe`, `mean`, `median`, `std`, `var`, `min`, `max`, `sum`, `count`, and more for descriptive statistics.\n",
    "\n",
    "3. **SciPy**:\n",
    "\n",
    "- SciPy is a library for scientific computing and technical computing.\n",
    "- It builds on NumPy and provides additional functionality for optimization, integration, interpolation, and statistical functions.\n",
    "- SciPy's stats module includes various statistical functions for descriptive statistics, such as `describe`, `mean`, `median`, `std`, `var`, `skew`, `kurtosis`, and many more.\n",
    "\n",
    "4. **StatsModels**:\n",
    "\n",
    "- StatsModels is a library for statistical modeling and hypothesis testing.\n",
    "- It provides classes and functions for estimating statistical models and conducting statistical tests.\n",
    "- StatsModels offers descriptive statistics functions as well as tools for `regression analysis`, `time series analysis`, and more.\n",
    "\n",
    "5. **Seaborn**:\n",
    "\n",
    "- Seaborn is a data visualization library based on Matplotlib.\n",
    "- It provides a high-level interface for creating attractive statistical graphics.\n",
    "- Seaborn includes functions for `visualizing distributions`, `categorical data`, and `relationships between variables`, which can be useful for exploring data during descriptive statistics analysis.\n",
    "\n",
    "\n",
    "6. **Matplotlib**:\n",
    "\n",
    "- Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.\n",
    "- While Matplotlib is primarily known for its `plotting capabilities`, it also offers functions for basic `statistical analysis` and `visualization`.\n",
    "\n",
    "7. **Scikit-learn**:\n",
    "\n",
    "- Scikit-learn is a `machine learning` library that includes modules for `data preprocessing`, `classification`, `regression`, `clustering`, and more.\n",
    "- Although its primary focus is machine learning, Scikit-learn also provides tools for `feature selection`, `dimensionality reduction`, and `basic statistical analysis`.\n",
    "\n",
    "8. **mimesis**: \n",
    "- Is a Python library, which helps generate mock data for various purposes. \n",
    "\n",
    "- The library was written with the use of tools from the standard Python library, and therefore, it doesn’t have any side dependencies. \n",
    "\n",
    "- Currently the library supports 32 languages and 21 class providers, supplying various data.\n",
    "\n",
    "- It can be easily integrated into data processing pipelines or testing frameworks to generate realistic sample data for testing or analysis.\n",
    "\n",
    "9. The Python package **tableone** is a tool for creating summary statistics tables for your dataset. It's particularly useful for generating descriptive statistics for categorical and continuous variables, stratified by groups.\n",
    "\n",
    "- Customizing Output: The tableone package offers various customization options for the summary table, such as specifying which statistics to include, formatting the output, and adding test statistics for group comparisons. Experimenting with these options can help tailor the output to your specific needs.\n",
    "\n",
    "- Advanced Features: Beyond basic summary statistics, tableone offers advanced features such as subgroup analysis, stratification by multiple variables, and customization of statistical tests. Exploring these features can enhance the depth and complexity of your analyses.\n",
    "\n",
    "These libraries offer a wide range of functionalities for descriptive statistics, data manipulation, visualization, and more. Depending on your specific needs and preferences, you can choose one or more of these libraries to perform descriptive statistics analysis in Python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40868cca",
   "metadata": {},
   "source": [
    "## DataFrame\n",
    "\n",
    "In Python, a **DataFrame** is a `two-dimensional` labeled data structure provided by the Pandas library. It can be thought of as a `table` or a `spreadsheet-like` data structure where data is organized into rows and columns. Each column can have a different data type (e.g., integer, float, string), and each row represents a single observation or record. DataFrames are widely used for data manipulation, analysis, and exploration in data science, machine learning, and other domains due to their flexibility, ease of use, and powerful functionality. <br> Key features of a DataFrame include:\n",
    "\n",
    "- **Indexing**: Each row and column in a DataFrame has a label, called an index, which allows for easy and efficient access to individual elements.\n",
    "\n",
    "- **Column Operations**: DataFrames support various operations on columns, such as selecting, adding, deleting, and renaming columns.\n",
    "\n",
    "- **Data Alignment**: DataFrames automatically align data based on labels, making it easy to perform operations on data with different indexes.\n",
    "\n",
    "- **Missing Data Handling**: DataFrames provide built-in support for handling missing data, allowing for flexible data manipulation and analysis.\n",
    "\n",
    "- **Grouping and Aggregation**: DataFrames support grouping data based on one or more columns and performing aggregation functions (e.g., sum, mean, count) on grouped data.\n",
    "\n",
    "- **Data I/O**: DataFrames can easily read data from and write data to various file formats, including CSV, Excel, SQL databases, and more.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c880c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DataFrame Example\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Creating a DataFrame from a dictionary\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emily'],\n",
    "    'Age': [25, 30, 35, 40, 45],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'San Francisco']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"DataFrame:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887ceb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to import excel data that has irrelevant rows.\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your Excel file\n",
    "excel_file = 'C:\\\\Users\\\\yawus\\\\OneDrive\\\\Desktop\\\\DS Notes\\\\notes\\\\visualization\\\\Canada.xlsx'\n",
    "\n",
    "# Specify the name of the sheet/tab you want to import\n",
    "sheet_name = 'Regions by Citizenship'\n",
    "\n",
    "# Read the specific sheet, skipping the first 20 rows and specifying the header names\n",
    "# `header=0` explicitly tells Pandas to use the first row as the header (column names) of the DataFrame.\n",
    "df = pd.read_excel(excel_file, sheet_name=sheet_name, skiprows=range(20), header=0)\n",
    "\n",
    "# Display the top 10 rows of the DataFrame\n",
    "print(df.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4cddd5",
   "metadata": {},
   "source": [
    "## Transfrom the DataFrame from Wide to Long:\n",
    "\n",
    "You can see that the Dataframe has dates that are not under one column as the other observations.\n",
    "\n",
    "To transform it to have the dates under a date column, we follow these steps using Python and Pandas:\n",
    "\n",
    "1. Read the data from Excel into a Pandas DataFrame.\n",
    "\n",
    "2. Reshape the DataFrame using the melt() function to unpivot the years into a single column.\n",
    "\n",
    "3. Rename the columns for clarity.\n",
    "\n",
    "4. Convert the \"Date\" column values to datetime format.\n",
    "\n",
    "5. Optionally, you can sort the DataFrame by the \"Date\" column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7919879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Melt the DataFrame from wide to long format, excluding the 'Type', 'Coverage', 'AreaName', and 'RegName' columns\n",
    "df_long = pd.melt(df, id_vars=['Type', 'Coverage', 'AreaName', 'RegName'], var_name='Year', value_name='Value')\n",
    "\n",
    "# Remove rows with '..' values\n",
    "df_long = df_long[df_long['Value'] != '..']\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df_long.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b4fc87",
   "metadata": {},
   "source": [
    "## Check if a Library is Installed.\n",
    "\n",
    "To use a library, it must be installed. It is important to check if a libaray is already installed to avoid installing it again.\n",
    "\n",
    "Installing a library that is already installed generally does not cause any harm. However, it can consume unnecessary time and bandwidth, especially if the library is large or if you are working with limited internet connectivity.\n",
    "\n",
    "While it's generally safe to install a library that is already installed, it's a good practice to periodically check for updates to your installed libraries and only install new libraries when necessary. Additionally, consider using virtual environments or package management tools like pip to manage your dependencies efficiently and avoid unnecessary installations\n",
    "\n",
    "Here are a few considerations:\n",
    "\n",
    "1. **Resource Utilization**: Installing a library consumes system resources such as disk space and memory. If you repeatedly install the same library unnecessarily, it can gradually consume significant resources over time.\n",
    "\n",
    "2. **Network Bandwidth**: Repeatedly downloading and installing the same library can consume network bandwidth, especially if you are working with limited or metered internet connections.\n",
    "\n",
    "3. **Version Control**: Installing the same library multiple times might result in different versions being installed across different environments, which could lead to compatibility issues or inconsistencies in your codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017bd3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if a library is installed\n",
    "\n",
    "# Check if NumPy is already installed\n",
    "try:\n",
    "    import numpy\n",
    "    print(\"NumPy is already installed.\")\n",
    "except ImportError:\n",
    "    print(\"NumPy is not installed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4260dc",
   "metadata": {},
   "source": [
    "You can repeat this process for each library you want to check. If you have a long list of libraries to check, you can create a function to simplify the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5bd231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_library(library_name):\n",
    "    try: # Block that executes code that might raise an exception.\n",
    "        __import__(library_name)\n",
    "        print(f\"Library '{library_name}' is already installed.\")\n",
    "    except ImportError: # This block executes If an `ImportError` occurs during the `try` block\n",
    "        print(f\"Library '{library_name}' is not installed.\")\n",
    "\n",
    "# Example usage\n",
    "check_library(\"numpy\")\n",
    "check_library(\"pandas\")\n",
    "check_library(\"matplotlib\")\n",
    "check_library(\"SciPy\")\n",
    "check_library(\"StatsModels\")\n",
    "check_library(\"Seaborn\")\n",
    "check_library(\"Scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aef45d",
   "metadata": {},
   "source": [
    "## Installing Libraries\n",
    "\n",
    "If a library or some libraries are not installed, then they can't be imported. You can install one library or severall libraries\n",
    "using the `pip install` command. Here is how to do it:\n",
    "\n",
    "`!pip install library1 library2 library3 ... libraryN`\n",
    "\n",
    "Additionally, you can specify versions for each library:\n",
    "\n",
    "!pip install library1==version1 library2==version2 library3==version3\n",
    "\n",
    "#### Example:\n",
    "\n",
    "!pip install numpy==1.20.3 pandas==1.3.2 matplotlib==3.4.3\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df4491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install The tableone package\n",
    "!pip install tableone\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f8507d",
   "metadata": {},
   "source": [
    "## How to  use `tableone`\n",
    "\n",
    "- Importing the necessary libraries:\n",
    "\n",
    "import pandas as pd\n",
    "from tableone import TableOne\n",
    "\n",
    "- Load your dataset:\n",
    "\n",
    "- Load your dataset into a pandas dataframe\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "- Create a TableOne object:\n",
    "\n",
    "\n",
    "table = TableOne(data=df, \n",
    "                 columns=['age', 'sex', 'bmi', 'outcome'], \n",
    "                 categorical=['sex', 'outcome'], \n",
    "                 nonnormal=['age', 'bmi'], \n",
    "                 groupby='outcome')\n",
    "\n",
    "- Generate the summary table:\n",
    "\n",
    "print(table)\n",
    "\n",
    "- Customizing the summary table:\n",
    "\n",
    "table_customized = TableOne(data=df, \n",
    "                            columns=['age', 'sex', 'bmi', 'outcome'], \n",
    "                            categorical=['sex', 'outcome'], \n",
    "                            nonnormal=['age', 'bmi'], \n",
    "                            groupby='outcome',\n",
    "                            labels={'age': 'Age (years)', 'bmi': 'BMI (kg/m^2)'},\n",
    "                            pval=True,\n",
    "                            formatStr='{:.2f}')\n",
    "\n",
    "- print(table_customized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cc574d",
   "metadata": {},
   "source": [
    "## Nota Bene\n",
    "\n",
    "In Jupyter Notebook, you typically do not need to import a library that has already been installed before you can use it. Once a library is installed in your Python environment, you can use it directly in any code cell without explicitly importing it again.\n",
    "\n",
    "However, there are a few scenarios where you might need to import a library even if it's already installed:\n",
    "\n",
    "1. **Namespace Collision**: If you have multiple versions of the same library installed, or if you have a variable with the same name as the library, you may need to import the library using a different name or alias to avoid namespace collisions.\n",
    "\n",
    "- import pandas as pd  # Importing Pandas with alias 'pd'\n",
    "\n",
    "\n",
    "2. **Restarting the Kernel**: If you restart the Jupyter Notebook kernel, you'll need to import the libraries again in the new kernel session before using them.\n",
    "\n",
    "\n",
    "3. **Cell Execution Order**: If you're running code in multiple cells and the cell containing the import statement is executed after cells that use the library, you'll need to import the library before using it in those cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc440825",
   "metadata": {},
   "source": [
    "# Notes - Getting the Data Ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4645ddb9",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d428e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xlrd is a package for reading excel files\n",
    "#!pip install xlrd \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mimesis\n",
    "from scipy import stats as st\n",
    "import xlrd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080f0f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your Excel file\n",
    "path = r'C:\\Users\\yawus\\OneDrive\\Desktop\\DS Notes\\notes\\descriptive statistics\\Sample - Superstore.xls'\n",
    "\n",
    "# Specify the name of the sheet/tab you want to import\n",
    "tab_name = 'Orders'\n",
    "\n",
    "# Import the file\n",
    "df = pd.read_excel(path, sheet_name = tab_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1e323a",
   "metadata": {},
   "source": [
    "### Make sure the data has been imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea96fbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get the top 5 rows of the data using the head() function\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197717ce",
   "metadata": {},
   "source": [
    "# Get the column names:\n",
    "Sometimes you will have a long list of columns. <br> Listing the columns is important to help know which columns to modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc1fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = list(df.columns)\n",
    "print(column_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef45d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative method for getting column names\n",
    "column_names = df.keys()\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe65503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another method for column names\n",
    "column_names = df.columns\n",
    "print(\"Column names:\")\n",
    "for col in column_names:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a00df35",
   "metadata": {},
   "source": [
    "### Get Data Types of Columns: <br > \n",
    "To retrieve the data types of each column, you can use the dtypes attribute of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26567bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = df.dtypes\n",
    "print(data_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e77e0b",
   "metadata": {},
   "source": [
    "### View Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5959f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158a841",
   "metadata": {},
   "source": [
    "### Data Type Details:\n",
    "- Sales : Numeric Continuous\n",
    "- Discount: Numeric Continuous\n",
    "- Profit: Numeric Continuous\n",
    "- Quantity: Numeric Discreet\n",
    "- The object is non-numeric - categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61662df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Numeric Columns to Strings\n",
    "\n",
    "df['Row ID'] = df['Row ID'].astype(str)\n",
    "df['Postal Code'] = df['Postal Code'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72d5c10",
   "metadata": {},
   "source": [
    "### Row Index:\n",
    "\n",
    "- To get the row index (labels), you can use the `.index` attribute.\n",
    "- It returns a pandas Index object containing the row labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e81be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = df.index\n",
    "print(\"Row index:\")\n",
    "for idx in row_index:\n",
    "    print(idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022fecdc",
   "metadata": {},
   "source": [
    "### Get the number of rows and columns in the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db643950",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49557a17",
   "metadata": {},
   "source": [
    "- This shows (# of rows, # of columns)\n",
    "- There are 9994 rows and 21 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b73715c",
   "metadata": {},
   "source": [
    "### Check for Missing Values in the Data\n",
    "\n",
    "1. **Using** `.isnull()` or `.isna()`:\n",
    "- Both .isnull() and .isna() return a DataFrame of the same shape as the original, <br> with True indicating missing values (NaN or None) and False indicating non-missing values.\n",
    "- You can then use `.sum()` to count the missing values per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4be2613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .isnull()\n",
    "# Check for missing values\n",
    "\n",
    "missing_values = df.isnull()\n",
    "print(\"Missing values (True indicates missing data):\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41438083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum missing values: 0 means no missing values \n",
    "missing_counts = df.isnull().sum()\n",
    "print(\"\\nCount of missing values per column:\")\n",
    "print(missing_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36811d17",
   "metadata": {},
   "source": [
    "2. **Using** `.any()`:\n",
    "- To check if any missing values exist in the entire DataFrame, <br> you can use .any() along the specified axis (default is axis=0 for columns).\n",
    "- It returns True if any missing value is found, otherwise False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8ad644",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_missing_values = df.isnull().values.any()\n",
    "print(\"\\nDoes the DataFrame have any missing values?\")\n",
    "print(has_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b74c342",
   "metadata": {},
   "source": [
    "## Descriptive Statistics\n",
    "\n",
    "**Descriptive statistics** refer to the analysis, summary, and presentation of findings related to a data set derived from a sample or the entire population. These statistics help us understand and describe the characteristics of the data.<br> The key components of descriptive statistics are: <br>\n",
    "- Frequency Distribution\n",
    "- Measures of Central Tendency\n",
    "- Measures of Variability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec1c9b9",
   "metadata": {},
   "source": [
    "The `statistics` module in Python provides functions for calculating mathematical statistics of numeric data. <br> Although it’s not meant to compete with third-party libraries like NumPy or SciPy, it serves well for basic statistical calculations. Here are some key features of the `statistics` module:<br> \n",
    "\n",
    "1. **Averages and Measures of Central Location**:\n",
    "- `mean(data)`: Computes the arithmetic mean (average) of data.\n",
    "- `fmean(data, weights=None)`: Calculates the floating-point arithmetic mean with optional weighting.\n",
    "- `geometric_mean(data)`: Computes the geometric mean of data.\n",
    "- `harmonic_mean(data)`: Calculates the harmonic mean of data.\n",
    "- `median(data)`: Finds the median (middle value) of data.\n",
    "- `median_low(data)` and median_high(data): Compute the low and high medians of data.\n",
    "- `median_grouped(data, interval=1)`: Calculates the median (50th percentile) of grouped data.\n",
    "- `mode(data)`: Determines the single mode (most common value) of discrete or nominal data.\n",
    "- `multimode(data)`: Provides a list of modes (most common values) for discrete or nominal data.\n",
    "- `quantiles(data, n=4, method='exclusive')`: Divides data into intervals with equal probability.\n",
    "\n",
    "2. **Measures of Spread**:\n",
    "- `pstdev(data)`: Computes the population standard deviation of data.\n",
    "- `pvariance(data)`: Calculates the population variance of data.\n",
    "- `stdev(data)`: Computes the sample standard deviation of data.\n",
    "- `variance(data)`: Calculates the sample variance of data.\n",
    "\n",
    "The `statistics` module is particularly useful for basic statistical tasks, but for more advanced analyses, consider using specialized libraries like `NumPy` or `SciPy`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499696d6",
   "metadata": {},
   "source": [
    "### Summary Statistics\n",
    "\n",
    "The `describe()` funstion in pandas is used to generate descriptive statistics. <br> For numeric columns, the output includes:\n",
    "- `count`: the number of non-na/null observations\n",
    "- `mean`: the average\n",
    "- `std`: the standard deviation\n",
    "- `min`: the minimum\n",
    "- `25%`, `50%`, `75%`:(lower quartile, median, and upper quartile)\n",
    "- `max`: maximum value\n",
    "\n",
    "For non-numeric columns ( strings or timestamps), the output includes:\n",
    "- `count`: number of non-na/null observations\n",
    "- `unique`: number of unique values\n",
    "- `top`: most common value\n",
    "- `freq` frequency of the most common value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acbd705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Stats for Numeric Variables\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec30582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics for non-numeric columns: use include = 'object'\n",
    "df.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a820b5b4",
   "metadata": {},
   "source": [
    "### CentralTendency - Mean, Median, Mode\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87139e4e",
   "metadata": {},
   "source": [
    "#### Mean of one Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4214be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sales_mean =df['Sales'].mean()\n",
    "sales_mean_rounded = round(sales_mean, 3)\n",
    "print(sales_mean_rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62257ed",
   "metadata": {},
   "source": [
    "#### Mean of two or more variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a7a45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "column_names =['Sales', 'Profit']\n",
    "mean_Sales_Profit = df[column_names].mean()\n",
    "print(\"Means of columns {} are:\\n{}\".format(column_names, mean_Sales_Profit))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452e41ce",
   "metadata": {},
   "source": [
    "#### Mean of Each Numeric Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80205ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean = df.mean(numeric_only = True)\n",
    "\n",
    "print(\"Means of numeric columns are:\\n{}\".format(mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c61c9b",
   "metadata": {},
   "source": [
    "We can calculate the mean for each row by supplying the axis argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e9830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the valid numeric columns\n",
    "numeric_columns = df.select_dtypes(include='number')\n",
    "\n",
    "# Calculate the mean across rows\n",
    "row_means = numeric_columns.mean(axis=1)\n",
    "\n",
    "print(\"Means across rows are:\\n{}\".format(row_means))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864dd071",
   "metadata": {},
   "source": [
    "#### Median of Each Numeric Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d6b9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "median = df.median(numeric_only = True)\n",
    "\n",
    "print(\"Medians of numeric columns are:\\n{}\".format(median))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179fcf4b",
   "metadata": {},
   "source": [
    "#### Mode of Each Numeric Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d0a57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = df.mode(numeric_only = True)\n",
    "print(mode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cc4944",
   "metadata": {},
   "source": [
    "### The 5 Summary Statistics\n",
    "The five summary statistics, also known as the five-number summary, is a set of descriptive statistics that provides information about a dataset. <br> It consists of the following five most important sample percentiles:\n",
    "\n",
    "- **Minimum**: The smallest observation in the dataset1.\n",
    "- **First Quartile (Q1)**: This is the value below which 25% of the data falls1.\n",
    "- **Median**: This is the middle value that separates the higher half from the lower half of the data set1.\n",
    "- **Third Quartile (Q3)**: This is the value below which 75% of the data falls1.\n",
    "- **Maximum**: The largest observation in the dataset1.\n",
    "These five statistics provide a concise representation of the distribution of a dataset, <br>offering insight into the central tendency, variability, and shape of the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f27678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "five_num = [df['Sales'].quantile(0).round(3),\n",
    "            df['Sales'].quantile(0.25),\n",
    "            df['Sales'].quantile(0.50).round(3),\n",
    "            df['Sales'].quantile(0.75),\n",
    "            df['Sales'].quantile(1)]\n",
    "five_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a760764d",
   "metadata": {},
   "source": [
    "### Histogram for Discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1386e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# You can adjust the number of bins to change the granularity of the histogram\n",
    "plt.hist(df['Discount'], bins= 15,color='green', edgecolor='black')\n",
    "\n",
    "plt.xlabel('Discount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Discount')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b129dd06",
   "metadata": {},
   "source": [
    "#### Distribution Curves for Sales\n",
    "\n",
    "The mean of 229.86 is far greater than the median of 54.49. The variable is therefore skewed right.<br> The median is a better representative of the center compared to the mean. There are outliers. We can confirm this with a distribution plot.<br> Also the maximum sales is 22638.48 and the minimum sales is 0.444. The range is 22638.48-0.444 = 22638.036"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2abcc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'df' is your DataFrame containing the 'sales' column\n",
    "\n",
    "# Create a figure and axis object\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot the distribution curve for the 'sales' variable\n",
    "sns.kdeplot(data=df['Sales'], ax=ax, label='Sales', fill=True)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Sales')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Distribution of Sales')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1277f7",
   "metadata": {},
   "source": [
    "The plot confirms that Sales is right skewed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c25492",
   "metadata": {},
   "source": [
    "### Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7807901",
   "metadata": {},
   "source": [
    "#### Print out Categorical Variables Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter DataFrame to include only categorical variables\n",
    "categorical_df = df.select_dtypes(include=['object'])\n",
    "\n",
    "print(categorical_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd72033",
   "metadata": {},
   "source": [
    "#### Print Out the Unique Categories in the 'Product Name. Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409ea084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Unique Categories\n",
    "unique_SubCategory = df['Sub-Category'].unique()\n",
    "\n",
    "# Print Unique Categories\n",
    "\n",
    "print(\"Unique Sub-Category :\\n\", unique_SubCategory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ebbdd6",
   "metadata": {},
   "source": [
    "#### Counts and Percentages\n",
    "- Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ee6d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts of 'Sub-Category' Cases\n",
    "SubCategory_counts = df['Sub-Category'].value_counts()\n",
    "print('Sub-Category :\\n' , SubCategory_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3b1d53",
   "metadata": {},
   "source": [
    "- Percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ade4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the counts of each subcategory\n",
    "SubCategory_Counts = df['Sub-Category'].value_counts()\n",
    "\n",
    "# Calculate the total number of observations\n",
    "n = len(df)\n",
    "\n",
    "# Calculate the percentage of each subcategory observation with 2 decimal places\n",
    "SubCategory_Percent = SubCategory_Counts / n * 100\n",
    "\n",
    "# Round the percentages to 2 decimal places\n",
    "SubCategory_Percent_rounded = SubCategory_Percent.round(2)\n",
    "\n",
    "# Convert the percentages to strings with the desired format\n",
    "SubCategory_Percent_str = SubCategory_Percent_rounded.astype(str) + '%'\n",
    "\n",
    "# Print the percentage of each subcategory observation with 2 decimal places\n",
    "print(f'Percent of Each Subcategory Observation:\\n{SubCategory_Percent_str}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bce3fa",
   "metadata": {},
   "source": [
    "- Percent for Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923e6c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count\n",
    "CategoryCount = df['Category'].value_counts()\n",
    "# Percent\n",
    "CategoryPercent = (CategoryCount / n) * 100\n",
    "\n",
    "# Convert Float to String\n",
    "\n",
    "print(CategoryPercent.round(3).astype(str) + \" %\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f2da9e",
   "metadata": {},
   "source": [
    "### Mode for Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b280525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mode for the specified columns\n",
    "mode = df[['Ship Mode','City', 'State','Product ID','Category','Sub-Category' ,'Segment']].mode(axis='rows', numeric_only=False)\n",
    "print(\"Mode for categorical variables:\")\n",
    "print(mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e281415",
   "metadata": {},
   "source": [
    "The mode is showing the observation with the highest frequency for each categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2964347",
   "metadata": {},
   "source": [
    "### Frequency Table & Bar Graph for Ship Mode Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95cab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a frequency table\n",
    "ship_mode_freq =df['Ship Mode'].value_counts()\n",
    "\n",
    "print('The Ship Mode Counts Are:')\n",
    "print(ship_mode_freq)\n",
    "\n",
    "# Plot a bar graph\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "ship_mode_freq.plot(kind='bar')\n",
    "plt.title('Frequency of Ship Mode')\n",
    "plt.xlabel('Ship Mode')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809be07a",
   "metadata": {},
   "source": [
    "The bar graph by frequencies confirm the mode that we calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f43de99",
   "metadata": {},
   "source": [
    "#### Create a Frequency Table and  a Bar Graph for Category Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91840775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list to pandas Series\n",
    "category_series = pd.Series(df.Category)\n",
    "\n",
    "# Calculate value counts\n",
    "value_counts = category_series.value_counts()\n",
    "\n",
    "# Calculate proportions\n",
    "proportions = value_counts / len(category_series)\n",
    "\n",
    "# Calculate percentages\n",
    "percentages = proportions * 100\n",
    "\n",
    "# Create a DataFrame for the frequency table with proportion and percent\n",
    "frequency_df = pd.DataFrame({'Frequency': value_counts, \n",
    "                             'Proportion': proportions, \n",
    "                             'Percent': percentages})\n",
    "\n",
    "# Sort the DataFrame by frequency in descending order\n",
    "frequency_df = frequency_df.sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "# Display the frequency table\n",
    "print(\"Frequency Table with Proportion and Percent:\")\n",
    "print(frequency_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77844432",
   "metadata": {},
   "source": [
    "### Bar Grap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d14ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "frequency_df['Frequency'].plot(kind='bar', color='skyblue')\n",
    "plt.title('Frequency of Categories')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d1550e",
   "metadata": {},
   "source": [
    "The graph show that Office Suplies has the highest frequency as shown by the mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5168ec",
   "metadata": {},
   "source": [
    "- `plt.figure(figsize=(10, 6))`: This line creates a new figure object with a specific size. <br>The figsize parameter specifies the width and height of the figure in inches. In this case, the figure size is set to 10 inches wide and 6 inches tall. \n",
    "\n",
    "<br>\n",
    "\n",
    "- `frequency_df['Frequency'].plot(kind='bar', color='skyblue')`: This line plots a bar <br> graph of the 'Frequency' column from the frequency_df DataFrame. <br> The `kind='bar'` parameter specifies the type of plot to create, which is a bar plot in this case.<br> The `color='skyblue'` parameter sets the color of the bars to sky blue.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "- `plt.title('Frequency of Categories')`: This line sets the title of the plot to 'Frequency of Categories'.\n",
    "\n",
    "<br>\n",
    "\n",
    "- `plt.xlabel('Category')`: This line sets the label for the x-axis to 'Category'.\n",
    "\n",
    "<br>\n",
    "\n",
    "- `plt.ylabel('Frequency')`: This line sets the label for the y-axis to 'Frequency'.\n",
    "\n",
    "<br>\n",
    "\n",
    "- `plt.xticks(rotation=45)`: This line rotates the x-axis tick labels by 45 degrees. <br> This is useful when you have long category names and want to avoid overlap between them.\n",
    "\n",
    "<br>\n",
    "\n",
    "- `plt.tight_layout()`: This line adjusts the layout of the plot to make sure all elements fit properly within the figure area, preventing overlap.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "- `plt.show()`: This line displays the plot. It's necessary to include this line to actually see the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e62090f",
   "metadata": {},
   "source": [
    "### Measures of Variability\n",
    "\n",
    "**Measures of variability** describe how data points spread out from each other and from the center of a distribution. <br> These statistics help us understand the dispersion or scatter in a dataset.\n",
    "\n",
    "- Range\n",
    "- Interquartile Range\n",
    "- Standard Deviation\n",
    "- Variance\n",
    "\n",
    "- Variability affects our ability to generalize results from a sample to a population.\n",
    "- Low variability allows better predictions based on sample data, while high variability makes predictions more challenging.\n",
    "- Both central tendency and variability together give a complete picture of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a1ba35",
   "metadata": {},
   "source": [
    "### Range\n",
    "\n",
    "- The range is the simplest measure of variability.\n",
    "- It tells us the **spread** of data from the **lowest** to the **highest** value.\n",
    "- To calculate the range, subtract the lowest value from the highest value in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb92c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Range for Sales\n",
    "\n",
    "Sales_Range = max(df['Sales']) - min(df['Sales'])\n",
    "print(Sales_Range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778d12d0",
   "metadata": {},
   "source": [
    "### Percentile : using the `quantile` function from numpy library\n",
    "\n",
    "- **Percentile**: Percentile is a specific value below which a given percentage of scores in a dataset fall.<br> For example, the 50th percentile (also known as the median) is the value below which 50% of the scores in the dataset fall. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d9d1a1",
   "metadata": {},
   "source": [
    "### Interquartile Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b21d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Interquartile Range\n",
    "Lower_Sales_Quartile = df['Sales'].quantile(0.25)\n",
    "Middle_Sales_Quartile = df['Sales'].quantile(0.50)\n",
    "Upper_Sales_Quartile = df['Sales'].quantile(0.75)\n",
    "Sales_IQR = Upper_Sales_Quartile - Lower_Sales_Quartile\n",
    "Ninety_Fith_Percentile = df['Sales'].quantile(0.95)\n",
    "\n",
    "print('Sales Lower Quartile is:', Lower_Sales_Quartile)\n",
    "print('Sales Median is:', Middle_Sales_Quartile.round(3))\n",
    "print('Sales Upper Quartile is:', Upper_Sales_Quartile)\n",
    "print('The Interquartile Range is:', Sales_IQR)\n",
    "print('The 95th Percentile is:', Ninety_Fith_Percentile.round(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1549c856",
   "metadata": {},
   "source": [
    "### Percentile - using the `scipy.stats.scoreatpercentile()`\n",
    "\n",
    "- This function computes the score at the given percentile of the input data. \n",
    "- Percentile: Percentile is a specific value below which a given percentage of scores in a dataset fall. \n",
    "- For example, the 50th percentile (also known as the median) is the value below which 50% of the scores in the dataset fall. --- Similarly, the 25th percentile (Q1) is the value below which 25% of the scores fall, <br> and the 75th percentile (Q3) is the value below which 75% of the scores fall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02e3fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "# Compute the median (50th percentile)\n",
    "\n",
    "median = stats.scoreatpercentile(df['Sales'], 50)\n",
    "\n",
    "# Compute the 25th and 75th percentiles\n",
    "q1 = stats.scoreatpercentile(df['Sales'], 25)\n",
    "q3 = stats.scoreatpercentile(df['Sales'], 75)\n",
    "\n",
    "print(\"Median:\", median.round(3))\n",
    "print(\"25th Percentile (Q1):\", q1)\n",
    "print(\"75th Percentile (Q3):\", q3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa2550",
   "metadata": {},
   "source": [
    "### Percentile Rank - using the `scipy.stats.percentileofscore()`: \n",
    "- This function computes the percentile rank of a score relative to a list of scores. Here's how to use it:\n",
    "- Percentile Rank: The percentile rank of a score is the percentage of scores in a dataset that are equal to or less than the given score. For example, if a score has a percentile rank of 75, it means that 75% of the scores in the dataset are less than or equal to this score.\n",
    "\n",
    "- Relative to a List of Scores: The function requires two main inputs: the list of scores (the dataset) and the specific score for which you want to calculate the percentile rank.\n",
    "\n",
    "- How to Use It: To use the function, you provide the list of scores as the first argument and the specific score for which you want to calculate the percentile rank as the second argument. The function then returns the percentile rank of the given score relative to the entire list of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2990ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Calculate the percentile rank of the score 95\n",
    "percentile_rank = stats.percentileofscore(df['Sales'], 95)\n",
    "sixty_first_percentile = df['Sales'].quantile(0.61)\n",
    "print(\"Percentile Rank of Score 95:\", percentile_rank.round(0))\n",
    "print('The 61st Percentile is:', sixty_first_percentile.round(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d287ed0d",
   "metadata": {},
   "source": [
    "- **Percentile Rank of Score 95: 61.0**: This means that a score of 95 is higher than 61% of all the scores in the distribution.<br> In other words, 61% of the scores are less than or equal to 95..\n",
    "- It does not provide information about the actual value at the 95th percentile.\n",
    "- **The 61st Percentile is: 95.0**: This means that 61% of all the scores in the distribution are less than or equal to 95. <br> In other words, a score of 95 is at the 61st percentile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a2707",
   "metadata": {},
   "source": [
    "### Standard Deviation\n",
    "- The standard deviation measures the average distance of data points from the mean (average).\n",
    "- It quantifies how much the data values deviate from the central tendency.\n",
    "- A larger standard deviation indicates greater variability.\n",
    "- It’s calculated using the square root of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7437387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Deviation of the Sales Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "numeric_columns = df.select_dtypes(include=np.number).columns\n",
    "\n",
    "# Calculate standard deviation for numeric columns\n",
    "std_devs = df[numeric_columns].std()\n",
    "\n",
    "print(\"Standard Deviations for Numeric Columns:\")\n",
    "print(std_devs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2227086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sales'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd36f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Standard Deviation Using statistics module\n",
    "#!pip install statistics\n",
    "import statistics\n",
    "\n",
    "std_dev = statistics.stdev(df['Sales'])\n",
    "std_dev2 = statistics.stdev(df['Profit'])\n",
    "print(std_dev)\n",
    "print(std_dev2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be531ae4",
   "metadata": {},
   "source": [
    "### Coefficient of Variation \n",
    "The Coefficient of Variation is defined as the ratio of the standard deviation to the mean of a dataset, expressed as a percentage. \n",
    "\n",
    "Here's how you can do it:\n",
    "\n",
    "- Using mean and standard deviation\n",
    "- Using variation function from scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e965dbb",
   "metadata": {},
   "source": [
    "#### Using mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e514fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sales_mean = np.mean(df['Sales'])\n",
    "profit_mean = np.mean(df['Profit'])\n",
    "\n",
    "sales_std_dev = np.std(df['Sales'])\n",
    "profit_std_dev =np.std(df['Profit'])\n",
    "\n",
    "sales_cv = (sales_std_dev/sales_mean)*100\n",
    "profit_cv =(profit_std_dev/profit_mean)*100\n",
    "\n",
    "print('The coefficient of variation for sales is :' , sales_cv)\n",
    "print('The coefficient of variation for profit is :', profit_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4b4e41",
   "metadata": {},
   "source": [
    "- Even though the standard deviation of Sales is greater than the standard deviation of Profit\n",
    "- The coefficient of variation for Profit is higher than the coefficient of variation for Sales\n",
    "- This means the Profit data points are more spread out compared to the Sales data points;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c322abe",
   "metadata": {},
   "source": [
    "#### Using variation function from scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe64d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import variation\n",
    "\n",
    "sales_cv = variation(df['Sales'])*100\n",
    "\n",
    "profit_cv = variation(df['Profit'])*100\n",
    "\n",
    "print(sales_cv)\n",
    "print(profit_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3c0542",
   "metadata": {},
   "source": [
    "### Variance\n",
    "- Variance is the average of squared distances from the mean.\n",
    "- It provides a measure of how much the data points vary from the mean.\n",
    "- Variance = average of (data value - mean)^2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689318b8",
   "metadata": {},
   "source": [
    "#### Variance by numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89baf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "variance = np.var(df['Sales'])\n",
    "print(variance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256bd227",
   "metadata": {},
   "source": [
    "#### Variance by statistics module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d471e2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "variance = statistics.variance(df['Sales'])\n",
    "print(variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de62fb69",
   "metadata": {},
   "source": [
    "### Measures of Shape\n",
    "\n",
    "Measures of Shape describe how data points are distributed within a dataset. \n",
    "\n",
    "1.**Symmetric Distribution**:\n",
    "- A distribution is considered symmetric if both sides of the distribution mirror each other.\n",
    "- Examples include the Normal Distribution, Rectangular Distribution, and U-shaped distribution.\n",
    "- In a symmetric distribution, the mean, median, and mode coincide.\n",
    "- The Normal Distribution, in particular, exhibits symmetry and has its peak near the center1.\n",
    "\n",
    "2.**Skewed Distribution**:\n",
    "- Skewed distributions are asymmetrical.\n",
    "- They can be positively skewed (tail extends to the right) or negatively skewed (tail extends to the left).\n",
    "- Positively skewed distributions have a longer right tail, while negatively skewed distributions have a longer left tail.\n",
    "- Examples of skewed distributions include income distributions (often positively skewed) and response times (often negatively skewed)1.\n",
    "\n",
    "3.**Kurtosis**:\n",
    "- Kurtosis measures the peakedness or flatness of a distribution.\n",
    "- A distribution can be:\n",
    "- Leptokurtic: Tall and narrow peak (high kurtosis).\n",
    "- Platykurtic: Flat peak (low kurtosis).\n",
    "- Mesokurtic: Similar to a normal distribution (moderate kurtosis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bfa67d",
   "metadata": {},
   "source": [
    "#### Skewness & Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792a260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "df['Sales'].skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4eac91",
   "metadata": {},
   "source": [
    "The skew of 12.973 is greater than 1, thus the Sales variable is highly positicely skewed.\n",
    "\n",
    "- A distribution with zero skew is perfectly symmetrical. Its left and right sides are mirror images.\n",
    "- Skewness values between -0.5 and 0.5 indicate approximately symmetric data distributions.\n",
    "- Skewness values between -1 and -0.5 (negative skewed) or between 0.5 and 1 (positive skewed) <br> suggest moderately skewed data distributions.\n",
    "- Skewness values less than -1 (negative skewed) or greater than 1 (positive skewed) indicate highly skewed data distributions.\n",
    "- Highly skewed data may require special treatment or transformation before certain statistical procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sales'].kurtosis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8083e53c",
   "metadata": {},
   "source": [
    "The kurtosis of 305.3 is greater than 0, the the Sales variable is **leptokurtic**: <br> The distribution has heavier tails (more extreme values) than a normal distribution.\n",
    "\n",
    "- A distribution with less tha 0 kurtosis is **platykurtic**: The distribution has lighter tails than a normal distribution (fewer extreme values).\n",
    "- A distribution with kurtosis equal to 0 is **mesokurtic**. The distribution has tails similar to a normal distribution.\n",
    "- A distribution with kurtosis less than 0 is **leptokurtic**. he distribution has heavier tails (more extreme values) than a normal distribution.<br> \n",
    "\n",
    "When assessing kurtosis, we often adjust the values by subtracting 3 to account for the normal distribution’s baseline. Therefore, a kurtosis value between **-3 and 3** (after this adjustment) is generally considered acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6615584",
   "metadata": {},
   "source": [
    "### Symmetric Distribution:\n",
    "Symmetric data refers to a distribution where the values are evenly distributed around a central point.\n",
    "1. **Characteristics of Symmetric Data**:\n",
    "- The mean (average), median, and mode are approximately equal.\n",
    "- The tails (extreme values) on both sides of the distribution have similar lengths.\n",
    "- The shape of the distribution looks balanced.\n",
    "\n",
    "2. **Importance of Symmetric Data**:\n",
    "- Statistical Assumptions: Many statistical methods assume that the data is symmetric or approximately normal (bell-shaped). For example:\n",
    "- Parametric Tests: Techniques like t-tests, ANOVA, and linear regression assume normality. Symmetric data aligns well with these assumptions.\n",
    "- Confidence Intervals: Symmetric data allows for accurate confidence interval estimation.\n",
    "- Ease of Interpretation: Symmetric distributions are easier to interpret. The mean, median, and mode provide consistent information about the central tendency.\n",
    "- Robustness: Symmetric data is more robust against outliers compared to skewed data.\n",
    "\n",
    "3. **Algorithm Assumptions**:\n",
    "- Many machine learning algorithms assume that the input features follow a symmetric or approximately normal distribution.\n",
    "- Symmetric data aligns with the assumptions of linear regression, logistic regression, and other parametric models.\n",
    "\n",
    "4. **Feature Scaling**:\n",
    "- Symmetric data simplifies feature scaling. Techniques like standardization (z-score normalization) work well when the data is symmetric.\n",
    "- Scaling ensures that features contribute equally to the model, improving convergence and stability.\n",
    "\n",
    "5. **Distance Metrics**:\n",
    "- Symmetric data enhances the effectiveness of distance-based algorithms (e.g., k-means clustering, k-nearest neighbors).\n",
    "- These algorithms rely on distance measures (such as Euclidean distance), which assume symmetric data.\n",
    "\n",
    "6. **Principal Component Analysis (PCA)**:\n",
    "- PCA, a dimensionality reduction technique, assumes symmetric data.\n",
    "- It transforms features into uncorrelated components, making it easier to capture essential information.\n",
    "\n",
    "7. **Decision Trees and Random Forests**:\n",
    "- While decision trees are not sensitive to data distribution, symmetric data often leads to better splits.\n",
    "- Random forests benefit from symmetric features during ensemble aggregation.\n",
    "\n",
    "8. **Interpretability**:\n",
    "- Symmetric data simplifies model interpretation.\n",
    "- Coefficients in linear models represent meaningful relationships when features are symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c26193",
   "metadata": {},
   "source": [
    "## How to fix Skew Data\n",
    "\n",
    "- Transformation\n",
    "- Normalization\n",
    "- Binning or Bucketing\n",
    "- Remove Outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23dc32d",
   "metadata": {},
   "source": [
    "### Transforming skew data to symmetric\n",
    "Data transformation involves altering the original data to improve its quality, distribution, or suitability for analysis. <br> It aims to make the data more amenable to modeling or statistical technique.\n",
    "\n",
    "- **Common Techniques**:\n",
    "    - **Log Transformation**: Applying the natural logarithm to data can help normalize skewed distributions.\n",
    "    - **Box-Cox Transformation**: A family of power transformations that optimally normalize data.\n",
    "    - **Square Root Transformation**: Useful for stabilizing variance and handling non-constant variance.\n",
    "    - **Other Power Transformations**: Including cube root, reciprocal, and exponential transformations.\n",
    "    \n",
    "- **When to Use**:\n",
    "    - When the data violates assumptions of normality or homoscedasticity.\n",
    "    - To reduce the impact of outliers.\n",
    "    - To linearize relationships between variables.\n",
    "\n",
    "The Sales variable is skewed. Let us transform it into a symmetric data.\n",
    "There are several methods:\n",
    "\n",
    "- **Using a defined function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5832ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform using square root\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def sqrt_transform(data):\n",
    "    return np.sqrt(data)\n",
    "\n",
    "# Apply to your skewed variable (e.g., Sale Price)\n",
    "transformed_sales = sqrt_transform(df['Sales'])\n",
    "\n",
    "print(transformed_sales.skew())\n",
    "print(transformed_sales.kurtosis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5b1aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform using log\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def log_transform(data):\n",
    "    return np.log(data)\n",
    "\n",
    "# Apply to your skewed variable (e.g., Sale Price)\n",
    "transformed_sales = log_transform(df['Sales'])\n",
    "\n",
    "print(transformed_sales.skew())\n",
    "print(transformed_sales.kurtosis())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61ed211",
   "metadata": {},
   "source": [
    "The log transform is better since the skew and kurtosis values are very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c783991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform using Box Cox\n",
    "import pandas as pd\n",
    "from scipy.stats import boxcox\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Assuming you have a dataframe named df with a column named 'Sales'\n",
    "\n",
    "# Transform using Box Cox\n",
    "transformed_data, lambda_value = boxcox(df['Sales'])\n",
    "\n",
    "# Convert transformed data to a Pandas Series\n",
    "transformed_series = pd.Series(transformed_data)\n",
    "\n",
    "# Calculate skewness and kurtosis of the transformed data\n",
    "skewness = skew(transformed_series)\n",
    "kurt = kurtosis(transformed_series)\n",
    "\n",
    "print(\"Skewness:\", skewness)\n",
    "print(\"Kurtosis:\", kurt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed728e5",
   "metadata": {},
   "source": [
    "The Box-Cox transformation is also doing a good job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4e3571",
   "metadata": {},
   "source": [
    "**Visualize the log transformed variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003971f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def log_transform(data):\n",
    "    return np.log(data)\n",
    "\n",
    "# Apply to your skewed variable (e.g., Sale Price)\n",
    "df[('transformed_sales')] = log_transform(df['Sales'])\n",
    "\n",
    "\n",
    "\n",
    "# Create a figure and axis object\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot the distribution curve for the 'sales' variable\n",
    "sns.kdeplot(data=df['transformed_sales'], ax=ax, label='Sales', fill=True)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('transformed_sales')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Distribution of transformed_sales')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673488f9",
   "metadata": {},
   "source": [
    "**Visualize the Box-Cox transformed variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b05c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import boxcox\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Assuming you have a dataframe named df with a column named 'Sales'\n",
    "\n",
    "# Transform using Box Cox\n",
    "transformed_data, lambda_value = boxcox(df['Sales'])\n",
    "\n",
    "# Add the transformed data to the dataframe\n",
    "df['Transformed_Sales'] = transformed_data\n",
    "\n",
    "# Plot the resulting distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot original data\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['Sales'], bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Original Sales Distribution')\n",
    "plt.xlabel('Sales')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Plot transformed data\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df['Transformed_Sales'], bins=20, color='lightgreen', edgecolor='black')\n",
    "plt.title('Transformed Sales Distribution')\n",
    "plt.xlabel('Transformed Sales')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f5dbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a distribution Curve for the Box-Cox Transfoemation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import boxcox\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Assuming you have a dataframe named df with a column named 'Sales'\n",
    "\n",
    "# Transform using Box Cox\n",
    "transformed_data, lambda_value = boxcox(df['Sales'])\n",
    "\n",
    "# Add the transformed data to the dataframe\n",
    "df['Transformed_Sales'] = transformed_data\n",
    "\n",
    "# Create a figure and axis object\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot the distribution curve for the 'sales' variable\n",
    "sns.kdeplot(data=df['Transformed_Sales'], ax=ax, label='Sales', fill=True)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Transformed_Sales')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Distribution of Transformed_Sales')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06a5a4c",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "The code **`plt.subplot(1, 2, 1)`** is a Matplotlib function used to create subplots in a figure.\n",
    "\n",
    "Here's what each argument represents:\n",
    "\n",
    "- **`1`**: The first argument specifies the total number of rows in the subplot grid.\n",
    "- **`2`**: The second argument specifies the total number of columns in the subplot grid.\n",
    "- **`1`**: The third argument specifies the index of the subplot to create.\n",
    "\n",
    "So, **`plt.subplot(1, 2, 1)`** is telling Matplotlib to create a subplot grid with 1 row and 2 columns,<br> and then to select the first subplot (from left to right) for plotting.\n",
    "\n",
    "In the context of the provided code for plotting histograms, **`plt.subplot(1, 2, 1)`** is used to specify that the first histogram (for the original data) should be plotted in the first subplot position. Similarly, **`plt.subplot(1, 2, 2)`** is used to specify that the second histogram (for the transformed data) should be plotted in the second subplot position. This allows for side-by-side comparison of the two distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf16b1fe",
   "metadata": {},
   "source": [
    "### Normalizing Skew Data\n",
    "Normalization ensures that features (variables) are on a similar scale, making them equally important <br> for machine learning algorithms. It doesn’t change the shape of the distribution but scales the data to a common range.\n",
    "\n",
    "- **Common Techniques**:\n",
    "    - **Standardization (Z-score normalization)**: Rescales features to have a mean of 0 and a standard deviation of 1. <br> Useful for optimization algorithms (e.g., gradient descent) and distance-based algorithms (e.g., K-nearest neighbors).\n",
    "    - **Max/Min Normalization (Min-Max Scaling)**: Transforms features to a range between 0 and 1.<br>  Minimum value becomes 0, and maximum value becomes 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b83ff62",
   "metadata": {},
   "source": [
    "**Standardization(Z-Score Normalization) Using Calculations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543ece55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and standard deviation of the 'Sales' column\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "sales_mean = df['Sales'].mean()\n",
    "sales_std = df['Sales'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707035b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply z-score normalization\n",
    "df['Sales_normalized'] = (df['Sales'] - sales_mean) / sales_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b37ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the DataFrame to verify the results\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbbd052",
   "metadata": {},
   "source": [
    "**Standardization(Z-Score Normalization) Using sklearn.preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b766007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the 'Sales' column and transform it\n",
    "df['Sales_normalized'] = scaler.fit_transform(df[['Sales']])\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify the results\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9068c7a2",
   "metadata": {},
   "source": [
    "**Notes on Noemalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa61f6f",
   "metadata": {},
   "source": [
    "Z-score normalization, also known as standardization, does not change the skewness of the data. Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. Z-score normalization shifts the mean of the data to 0 and scales it by the standard deviation, but it does not alter the shape of the distribution.\n",
    "\n",
    "If the original data is positively or negatively skewed, the normalized data will retain the same skewness. However, standardization can help make the data more suitable for certain statistical analyses or machine learning algorithms that assume the data is centered around 0 with a standard deviation of 1.\n",
    "\n",
    "If you want to reduce skewness in the data, you might consider applying transformations such as logarithmic transformation, square root transformation, or Box-Cox transformation before normalizing the data using z-score normalization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ae5b70",
   "metadata": {},
   "source": [
    "**Normalization Using Min/Max(Min-Max) Scaling**\n",
    "- Using Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf77514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the minimum and maximum values of the 'Sales' column\n",
    "min_sales = df['Sales'].min()\n",
    "max_sales = df['Sales'].max()\n",
    "\n",
    "# Perform Min-Max Scaling\n",
    "df['Sales_Scaled'] = (df['Sales'] - min_sales) / (max_sales - min_sales)\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify the results\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c479f6",
   "metadata": {},
   "source": [
    "- Using sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86da89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "#Fit the scaler to the 'Sales' column and transform it\n",
    "df['Sales_Scaled2'] = scaler.fit_transform(df[['Sales']])\n",
    "# Display the first few rows of the DataFrame to verify the results\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ab787",
   "metadata": {},
   "source": [
    "### Z-Score Normalization vs Scaling Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992b2e29",
   "metadata": {},
   "source": [
    "**Z-score normalization (Standardization)**:\n",
    "\n",
    "Z-score normalization scales the data so that it has a mean of 0 and a standard deviation of 1.\n",
    "It centers the data around the mean and scales it by the standard deviation.\n",
    "The resulting distribution will have a mean of 0 and a standard deviation of 1.\n",
    "Z-score normalization does not bound the data to a specific range.\n",
    "\n",
    "**Scaling (Min-Max Scaling)**:\n",
    "    \n",
    "Min-Max Scaling scales the data to a fixed range, typically between 0 and 1.\n",
    "It preserves the shape of the original distribution but scales the values to a specified range.\n",
    "The resulting distribution will have values bounded within the specified range.\n",
    "Min-Max Scaling does not center the data around a specific mean or standard deviation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907444ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
